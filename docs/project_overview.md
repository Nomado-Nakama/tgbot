# Implementation Plan for Telegram Bot with Inline Menus and Semantic Search

## Project Overview and Requirements

* **Goal:** Develop a Telegram bot (in **Python**, using **Aiogram** for async handling) that delivers informational content in Russian through a nested inline-button menu. Users can navigate a tree-structured content menu and also perform semantic search queries to find relevant info.
* **Key Features:** Multi-level inline keyboard navigation for content, an **embedding-based semantic search** (store content vectors in a vector DB for similarity lookup), an integrated admin **CMS** (within the bot) for content management, and support for multiple admin users. Data will be stored in a **PostgreSQL** database (content and menu structure) and a scalable **vector database** (e.g. Qdrant) for embeddings.
* **Deployment & Quality:** The project should use **Docker Compose** for easy deployment (bot, Postgres, Qdrant containers) and the code hosted on GitHub with a basic **CI pipeline** (GitHub Actions) to run style checks and tests. We will enforce code quality with formatting (Black) and linting (Ruff), use Alembic for DB migrations, and follow TDD principles (write tests for each feature to prevent regressions). The architecture must be volume-aware and scalable to serve content for 50+ countries (potentially high load), prioritizing clear structure, maintainability, and async best practices.

## Technology Stack and Architecture

* **Python & Async Framework:** Use **Aiogram 3.x**, a fully asynchronous Python framework for Telegram bots. Aiogram will efficiently handle concurrent updates (users pressing buttons, sending queries) without blocking the event loop. We will use **async IO** throughout (e.g. non-blocking HTTP requests and DB queries) to ensure the bot remains responsive under load.
* **Data Storage:** Use **PostgreSQL** for storing content and its tree hierarchy. Each content item (e.g. a country or a topic) will be a DB record containing text and a reference to its parent (to form the menu tree). We’ll manage schema changes with **Alembic** migrations for reliable DB versioning.
* **Semantic Search:** For semantic vector search, use **Qdrant** – an open-source vector database that provides efficient similarity search and **horizontal scalability** for growing data volumes. Each content piece’s text will be converted into an embedding vector (using a language model) and stored in Qdrant. Qdrant’s collections will hold these vectors with an ID or payload linking back to the content record. This setup allows fast semantic queries even as data grows, since Qdrant supports distributed scaling.
* **Bot Structure:** Organize the bot code into clear modules for maintainability. For example: a module for **user features** (menu navigation, search), another for **admin features** (content management), and a **data access layer** for DB interactions. Aiogram’s Router system will be used to separate handlers (e.g. an `admin_router` for admin-only commands and a `user_router` for public commands) which improves code clarity. We will maintain a list of admin user IDs (configured via env or config file) to restrict admin-only actions.
* **DevOps & CI:** The project will include configuration for **Black** and **Ruff** to automatically format code and catch lint issues. Black ensures a consistent coding style (PEP 8) automatically, and Ruff provides fast linting/analysis to keep code clean and error-free. A **GitHub Actions** CI workflow will run these tools and execute tests on every push, preventing bad code from entering main branches. We will use **Docker Compose** to define all services (bot, Postgres, Qdrant) for easy deployment and reproducible dev environment. Each service will be containerized, making it straightforward to launch the entire stack on a VPS.

## Milestone 0: Project Setup and Tooling

*This initial setup milestone establishes the project structure, development tools, and CI/CD pipeline.*

1. **Version Control and Repository:** Initialize a Git repository (host on GitHub) for the project. Create a basic README outlining the project and include a description of how to configure and run the bot. This helps collaborators (and future developers) understand the project intent and setup.
2. **Project Structure:** Create a clear Python package structure. For example:

   * `bot/` (Python package for bot code) with sub-packages:

     * `bot/admin/` – admin panel logic (handlers for content CRUD, etc.)
     * `bot/user/` – user-facing logic (handlers for commands, menu navigation, search)
     * `bot/models.py` (or a `bot/db/` package) – SQLAlchemy models for content, possibly DB session setup.
     * `bot/config.py` – configuration management (e.g. Pydantic models or `dataclasses` to load env vars).
     * `bot/main.py` – entry point to initialize the bot and register routers.
   * Tests structure: e.g. `tests/` directory mirroring the bot structure for unit tests (e.g. `tests/test_menu_navigation.py`, `tests/test_search.py`, etc.).
     This modular structure ensures separation of concerns (e.g. admin vs user logic) and improves maintainability.
3. **Dependency Setup:** Create a `pyproject.toml` or `requirements.txt` listing key dependencies: Aiogram, SQLAlchemy, asyncpg (Postgres async driver), qdrant-client, Pydantic, etc. Install them in the dev environment. Also include dev/test dependencies: **pytest** for testing, **black** for formatting, **ruff** for linting. Ensure the Python version is compatible (Python 3.10+ recommended for Aiogram 3).
4. **Bot Token Configuration:** Securely store the Telegram bot token (from BotFather) and other secrets. For local dev, use a `.env` file (git-ignored) with values like `BOT_TOKEN`, `DATABASE_URL`, etc. In `bot/config.py`, use Pydantic's BaseSettings or similar to load these variables, so configuration is centralized. This will also allow easily switching tokens for testing (e.g., using a test bot).
5. **Logging and Localization:** Set up basic logging (using Python’s `logging` module or Aiogram’s built-in logging utilities) to record info and errors. Since the bot is Russian-only for now, we can hard-code Russian text strings in messages. However, note this in the code, and consider structuring messages so that adding other languages later is possible (for example, keep all user-facing text in a single module or file for easy refactoring to support i18n).
6. **Code Style and Linting Tools:** Configure **Black** and **Ruff**. For Black, a simple configuration (line length, etc.) can be added to pyproject or use defaults (PEP8). For Ruff, create a `ruff.toml` or pyproject section to enable desired rules (Ruff combines many lint checks, e.g. flake8, isort, etc., in one). Set up a **pre-commit** hook (optional) to run Black and Ruff on commits for developer convenience. Ensure that running `black .` and `ruff .` in the project root produces no errors on the initial scaffold.
7. **Database Initialization:** Set up **Alembic** for database migrations. Run `alembic init` to create migration structure, and configure Alembic’s `alembic.ini`/env to connect to the Postgres database (use the `DATABASE_URL` from env). Write the first migration for the initial content schema (this will be done in Milestone 1 when we define the content model). For now, ensure Alembic is wired up so that `alembic upgrade head` works (even if it does nothing yet).
8. **Continuous Integration (CI):** Create a GitHub Actions workflow (YAML in `.github/workflows/ci.yml`). Include steps to:

   * Checkout code, set up Python, install dependencies.
   * Run Black in check mode and Ruff to lint the code. This ensures code style guidelines are enforced in CI.
   * Run Pytest to execute tests. Initially, there may be no tests or just a dummy test (you can add a trivial always-passing test to ensure the pipeline is green). As development progresses, this will catch regressions.
   * (Optional) Cache dependencies to speed up CI. Also possibly matrix builds for multiple Python versions if desired.
9. **Docker Compose Setup:** Start writing a `docker-compose.yml` that will later be used for deployment. For now, include a Postgres service (use the official image, set up environment for default user/password), and maybe a Qdrant service (official Qdrant image) placeholders. Also include a service for the bot (build from a Dockerfile). This can evolve, but having it early ensures that from the start we think about containerization. A simple `Dockerfile` for the bot can also be drafted (Python base image, copy code, install deps). We won’t fully use it until deployment, but setting it up now aids consistent environments.
10. **Verification:** As a smoke test for setup, implement a simple `/ping` or `/start` handler that replies with “Bot is alive” (in Russian) to verify that the bot can start and respond. Run the bot locally (perhaps with `python bot/main.py`) and interact via Telegram to ensure the basics are working. This is not yet the real functionality, but it tests that Aiogram is set up correctly with the token, and our structure is functional.

## Milestone 1: MVP – Basic Content Navigation Bot

*This milestone delivers a minimum viable product: the bot can present informational content in a nested menu via inline buttons. Content is stored in the database, and users (in private chat with the bot) can navigate through categories and read text. No search or admin editing yet – content is managed offline or via DB scripts.*

**Scope of MVP Features:**

* **User Commands:** Implement `/start` (and `/help`) commands. `/start` will send a welcome message (in Russian) and display the top-level menu (inline buttons for main categories). `/help` can simply reiterate usage (like “Use the buttons to navigate the information, or send a query to search.”).
* **Inline Menu Navigation:** Multi-level inline keyboard navigation for content hierarchy. Users can press a button to open subcategories or an article. A “Back” button allows going up one level, and perhaps a “Home” button to return to the main menu.
* **Content Data:** Define a content model in the database and use it to populate the menus. For MVP, the content can be pre-loaded (via a SQL script or initial migration) since we don't have admin creation yet. For instance, load information about each of the 50+ countries as top-level items (or grouped by region). Each item might have text (if it’s a leaf article) or be a category containing sub-items.

**Implementation Steps:**

1. **Design Content Schema:** Create a SQLAlchemy model `Content` for informational content. Fields may include: `id` (primary key), `parent_id` (nullable FK to Content.id, for tree hierarchy), `title` (short title for menu button, e.g. country name), `body` (text content, for leaf nodes; can be empty or null for purely grouping nodes), and maybe `order` (to sort siblings). You might also include a `type` or `is_category` flag, or infer category if `body` is null. Keep the schema minimal for MVP. Write an Alembic **migration** to create the `content` table with these fields. Run `alembic upgrade head` to apply it to the dev database.
2. **Data Access Layer:** Implement a small set of functions or a DAO class to interact with the content data. For example: `get_children(parent_id)` to retrieve all Content with given parent (or all top-level content if parent\_id is null), and `get_content(item_id)` to fetch a content item by id. These should use async SQLAlchemy queries or raw SQL via asyncpg. If using SQLAlchemy 2.0 ORM, set up an **async Session** and use `async with Session()...` in these functions to fetch data. This abstracts the DB logic away from bot handlers.
3. **Aiogram Bot Initialization:** In `bot/main.py`, initialize the Aiogram bot and dispatcher. Use the `Dispatcher` and possibly `Router` architecture: create a `user_router` for user-facing handlers and attach it to the dispatcher. Likewise, prepare an `admin_router` (even if admin features come later) to keep things modular. For now, only set up `user_router`.

   * Configure the bot with your token and parse\_mode (maybe HTML for rich text). Also decide on polling vs webhook: for development, polling is simpler; for production, we might switch to webhooks. MVP can use long polling.
   * Register the `/start` and `/help` command handlers. For example, use `@user_router.message(CommandStart())` for start. In the handler, call `get_children(parent_id=None)` to get top-level items and send a welcome message with an inline keyboard of these items.
   * Build inline keyboards using Aiogram’s `InlineKeyboardMarkup`. For each content item, create an `InlineKeyboardButton(text=title, callback_data=f"open_{id}")`. Also add a special button for “Help” or others if needed. Use Russian text (e.g., “Назад” for Back). The Aiogram 3 `InlineKeyboardBuilder` can help assemble the keyboard conveniently.
4. **Inline Callback Handling:** Implement a callback query handler to handle navigation button clicks. For example, if `callback_data` starts with `"open_"`, parse the ID after it. Fetch that content item from DB. Two cases:

   * If the item has children (i.e., is a category), send a new message (or edit the current message) with the list of children as a new inline keyboard. Also include a “Назад 🔙” button that triggers going back (perhaps with `callback_data="back_{parent_id}"`) and a “🏠 Главная” (Home) button to go to top-level (`callback_data="back_root"` or similar).
   * If the item has no children (a leaf with actual content text), send the content text as a message. If the text is long, consider sending multiple messages or using Telegram’s pagination (e.g., split by paragraphs) to avoid message length limits. Along with the content, still provide a “Back” inline button to go up to the parent menu (the parent\_id is known from the content record). Possibly also a “Home” button.
     Use Aiogram’s `@user_router.callback_query()` decorator with filters to match these patterns. For instance, `@user_router.callback_query(F.data.startswith("open_"))` can catch open requests. Similarly, handle `back_` actions. Maintain clean navigation: when a user clicks “Back”, we fetch the parent’s siblings again and show that menu.
5. **Multiple Users & State:** Aiogram will handle multiple users concurrently by design (each callback carries the user identity). We should ensure that our handlers don’t use any global mutable state for navigation; instead they compute menu content on the fly from the database. This way, any number of users can navigate the menu tree independently. The current "state" (which menu level the user is at) is essentially reflected by the messages and callback data, so we may not need to explicitly track user state for simple navigation. (If needed, Aiogram’s FSM could be used, but here the navigation is stateless request/response style).
6. **Russian Content and UX:** All bot messages should be in Russian. Ensure proper encoding and possibly use Unicode emojis for clarity (like arrows, home icon, etc., as in the examples above). Double-check that text formatting (if using Markdown or HTML for bold, etc.) works as expected in Telegram. Also verify that inline buttons text aren’t too long (Telegram has limits per button).
7. **Manual Content Seeding:** Before testing, insert some example content into the database so that menus have something to show. This can be done via an SQL script or through the PG console. For example, create a few top-level entries (e.g., “Европа”, “Азия”, etc. if grouping countries by region, or each country as top-level) and some children for one of them to test multi-level navigation. Alternatively, one can write a one-off Python script that uses SQLAlchemy to add records. This seeding step is temporary until the admin CMS is built.
8. **Testing Navigation (TDD approach):** Write tests to validate the menu logic and handlers:

   * **Unit Test DB logic:** Using an in-memory SQLite (for quick tests) or a test Postgres database, verify that `get_children` returns correct results given a known set of content records, and that hierarchy is represented properly (e.g., parent with children). These tests ensure the ORM queries or SQL are correct.
   * **Unit Test Keyboard Building:** Write a pure function that given a list of content items returns an InlineKeyboardMarkup. Test that this function outputs the expected structure (e.g., correct button texts and callback\_data). This can be done without running a bot.
   * **Simulate Callback Handling:** Aiogram’s design allows testing handler logic by calling the handler coroutine with a dummy `CallbackQuery` object. We can create a fake `CallbackQuery` (or use the `aiogram_tests` library to simulate one). For example, simulate a user clicking a button with data "open\_5" and assert that the bot (our handler) queries the DB for item 5 and prepares a reply. We might need to monkeypatch the `bot.send_message` in tests to capture what would be sent. Another approach is to use dependency injection for data access: e.g., pass a mock DAO into the handler to control what data is returned.
   * **End-to-End Manual Test:** In addition to automated tests, run the bot locally and use the Telegram client to navigate the menus. Confirm that you can go into sub-menus, back out, and read content. Check that no exceptions occur on the bot side (watch the logs), and that the UI/UX is smooth (no old inline keyboard left hanging, etc.).

Throughout this milestone, follow TDD where feasible: write a test for a piece of functionality, then implement the minimal code to pass it, then refactor. For example, write a test that `get_children(None)` returns the list of top items (after seeding known items), then implement `get_children`. This will help catch mistakes early and build confidence in the code’s correctness. We should also ensure our code passes linting/formatting checks at each step (possibly using a pre-commit hook to auto-format with Black on save, etc., to make development smoother).

## Milestone 2: Admin CMS for Content Management

*Now that the bot can display content, this milestone adds administrative capabilities to manage that content through the bot itself (acting as a mini CMS). Multiple admins should be supported, so we’ll restrict these features to authorized users. Key functions include adding new content, editing existing content, and deleting content.*

**Implementation Steps:**

1. **Admin Authentication & Access Control:** Introduce a mechanism to distinguish admins. Use a list of admin Telegram user IDs (perhaps in config env `ADMIN_IDS`). In Aiogram, we can **filter** handlers by user ID. For example, define `ADMIN_IDS = {12345678, ...}` and use a filter in handlers: `F.from_user.id.in_(ADMIN_IDS)` to guard admin-only commands. Alternatively, create a custom filter or use a simple `if message.from_user.id not in ADMIN_IDS: return` inside handlers. This ensures only designated users can perform content management actions.
2. **Admin Command & Menu:** Decide how admins will access the CMS features. A simple approach is to use bot commands for each action:

   * `/admin` – (optional) could show an admin help or menu of actions (though simple commands might suffice without a menu).
   * `/add` (or `/addcontent`) – add a new content node.
   * `/edit <id>` – edit an existing content node identified by ID.
   * `/delete <id>` – delete a content node (by ID).
     These commands can be documented for admins. Alternatively, implement an inline admin panel: e.g., when an admin sends /admin, respond with an inline keyboard: "Add Content", "Manage Content" etc., and build further interactions from there. For simplicity, command-based interactions (with possible conversational flow) are fine.
3. **Adding Content (Conversation):** Implement the logic for `/addcontent`. Adding an entry might require multiple pieces of info: the parent under which to add, the title, and the body text. We can leverage **Aiogram’s Finite State Machine (FSM)** to manage multi-step input from the admin. For example:

   * Admin sends `/addcontent`. Bot replies (only visible to admin) asking “Please send the title of the new content”. Set FSM state to `AddContent.title`.
   * Next message from admin is captured (text of title). Store it (FSM context or a temp dict) and advance state to `AddContent.body`. Ask admin for the body text (or say “send – or type 'none' if no body”).
   * Admin sends the body text. Store it. Now, ask for the parent category under which to place this content. This could be tricky as they need to specify parent. We could ask them to send the parent’s ID or name. For better UX, perhaps list top-level categories as inline buttons to choose from, then subcategories, etc. But this becomes a mini navigation in itself. Given the complexity, one approach is: if the bot content structure isn’t huge, the admin could navigate the same inline menus to where they want to add and press an "➕ Add here" button (only visible to admins). That would eliminate needing to ask parent via text. However, implementing that requires mixing admin options into user menu, which we can do by checking user ID in the menu builder. We can consider this enhancement:

     * Modify the menu generation in Milestone 1: if the requesting user is admin, append an extra button like "➕ Add new sub-item" in each category view, with callback `admin_add:<parent_id>`. Similar for perhaps an "✏️ Edit this" or "❌ Delete" on content items.
     * This approach makes admin actions more intuitive (just click where to add). It can be implemented using the same callback query mechanism but restricted to admins.
       For now, to keep it simpler: use text commands. We can implement `/addcontent <parent_id>` variant: admin invokes with a parent id (or 0 for root) as argument. If no argument, assume root or prompt for parent. Then ask for title and body as above. Using FSM, after collecting title and body, perform the DB insertion: create a new Content record with given parent, title, body.
   * After saving, send a confirmation to admin (and maybe automatically update the bot’s menu cache if we maintain one). If we have caching, we need to invalidate or update it. Initially, we might simply fetch from DB each time, so the new content will naturally appear.
4. **Editing Content:** Implement `/edit <id>`. Admin provides the ID of the content to edit (they might get this ID from the database or from a special admin view – we might need a way for them to know IDs, which is not user-friendly if they have to go to DB). A more user-friendly way: if we have the admin inline approach, an admin viewing a content item could click an "Edit" button that we provide. However, as a starting point, assume the admin knows or can retrieve the ID (perhaps we can build a command `/list` that dumps all content titles with IDs for admins). For simplicity:

   * When `/edit <id>` is received (and the user is admin), fetch that content from DB. Reply asking what to edit: "Send the new title, or /skip to keep current" then "Send new body, or /skip". This can be a two-step FSM (state for editing title, then state for editing body). Or we could allow specifying one or both via some syntax, but interactive is fine.
   * Update the fields in DB using an UPDATE query or ORM session. Then send admin a confirmation. If using caching for menus, invalidate the relevant parts.
     Another approach is to allow editing via an inline UI: show the current content and let admin send a new message to replace it. But that adds complexity, so stick to command & prompt.
5. **Deleting Content:** Implement `/delete <id>`. For safety, when admin sends this command, respond with a confirmation prompt (maybe inline buttons: "Confirm delete ID X" and "Cancel"). Only upon confirmation, delete the record from the DB (and possibly any children if it’s a category – ensure to define cascade rules or manually delete subtree). Use an Aiogram callback or a temporary FSM state for confirmation. After deletion, notify the admin of success.

   * Alternatively, use the inline method: an admin viewing a content item could press a "Delete" button which triggers a confirmation step. This could be done via callback query as well.
6. **Database Operations & Integrity:** Ensure that content addition/edit/deletion use transactions (if using SQLAlchemy session, commit after each change). For deletion, if a node with children is deleted, decide how to handle children – possibly delete all descendant nodes (cascading) to avoid orphaned entries. This could be a dangerous operation, so double-check the logic or restrict deletion only to leaves for now (optionally).

   * If we want soft-deletes or archival, that’s an extra complexity; likely not needed, hard delete is fine given an admin scenario.
   * Write an Alembic migration if we decide to add any new fields during this (for example, if we add a `created_at` or `updated_at` timestamp field to Content, or a separate table for admin users if we wanted to manage admins in DB rather than config). For MVP CMS, probably not needed.
7. **Permissions and Role Management:** The spec mentions multiple admin users. We assume all admins have equal privileges. If desired, one could implement different admin roles (like super-admin that can add other admins), but that's beyond scope for now. We simply maintain a list of authorized IDs. Possibly implement a command `/addadmin <user_id>` that the bot owner can use to update the config at runtime (if we stored admins in DB or a file). However, simpler is to keep it static or require code change for new admins, since adding that feature might be overkill for MVP.
8. **Testing Admin Features:** Apply TDD for admin features as well:

   * **Unit Test Add (logic):** If using a function `add_content(parent, title, body) -> Content`, test it with an in-memory DB to ensure it inserts correctly and returns an object with an ID. If using the bot FSM approach, test the individual steps: simulate the admin sending `/addcontent 0`, then sending "Test Title", then "Test Body", and assert that a new record exists. Aiogram’s testing can be tricky for full conversation flows, but we can test the underlying functions easily and perhaps simulate a single step of the handler with a given FSM state.
   * **Unit Test Edit/Delete:** Similarly, test that editing actually changes the DB field. For delete, test that deleting a content that has children either fails or removes children as expected (depending on design). Use a temporary test transaction or a rollback to not actually lose the test data permanently.
   * **Security tests:** Attempt to call admin commands as a non-admin in tests (simulate a message from a non-admin user ID for `/addcontent`) and assert that the bot either does nothing or returns an error like "Unauthorized". This ensures the admin check is effective.
   * **Manual Test via Telegram:** As an admin user, try adding a new content entry from within Telegram. Verify it appears in the menu for normal users. Test editing and deleting content live. Check that normal users do not see admin-only buttons (if you implemented "Add here" buttons in menu, for instance, log in as a normal user to confirm they are hidden). Also ensure that edge cases are handled (like adding content with an extremely long text, or deleting a node that has children – make sure the UI responds gracefully).

By the end of Milestone 2, content creators (admins) should be able to manage the informational content entirely through the bot interface, without direct database access. This completes the core functionality: regular users can navigate and read content, and admins can update it in real-time.

## Milestone 3: Semantic Search Integration

*This milestone adds the embedding-based search feature. Users will be able to enter free-text queries (in Russian) to search the knowledge base semantically, rather than just by keywords. This involves generating embeddings for each content piece, storing them in a vector database (Qdrant), and querying that DB for similar vectors.*

**Implementation Steps:**

1. **Embedding Generation Setup:** Choose an embedding model suitable for Russian semantic search. Options include:

   * External API like **OpenAI’s** text-embedding-ada-002, which produces 1536-dimensional vectors (high quality, but requires API calls and internet access).
   * A local **sentence-transformers** model (e.g. multilingual MiniLM or Distiluse-base-multilingual) which produces \~384-768 dimensional vectors. This avoids external dependencies but will consume memory and CPU.
   * Qdrant’s own **FastEmbed** library is another option (it provides ready-to-use models and workflows).
     For simplicity and given only Russian support, a model like `paraphrase-multilingual-MiniLM-L12-v2` (sentence-transformers) could be used. We'll integrate the chosen method into the bot.
     Implement a utility function `generate_embedding(text: str) -> list[float]` that uses the model/API to obtain the vector. Make sure this function is async if calling an API (e.g., use an `aiohttp` client to call OpenAI) or, if using a local model, consider offloading to a thread executor because model computation can be CPU-bound and slow. We may initialize the model globally at startup to avoid reloading it on each call.
2. **Initialize Qdrant Vector DB:** In the development environment, run a Qdrant instance (as per Docker Compose). Use the **qdrant-client** Python library to connect to Qdrant from the bot. On startup, the bot should ensure that a Qdrant *collection* exists for our content vectors. For example, name the collection "content\_vectors". Define the vector size equal to the embedding dimension (e.g., 384) and choose an appropriate distance metric (cosine is typical for embeddings). This can be done with something like:

   ```python
   from qdrant_client import QdrantClient
   qdrant = QdrantClient(url="qdrant:6333")  # if using docker compose service name
   qdrant.recreate_collection(collection_name="content_vectors", vectors_config=VectorParams(size=384, distance=Distance.COSINE))
   ```

   (During development, recreating is fine; in production, we’d not drop existing data). Ensure Qdrant is running and accessible. Qdrant supports various languages and has a REST API as well, but the Python client suffices.
3. **Index Existing Content:** We need to generate embeddings for all existing content and upload to Qdrant. This can be done in a one-time script or on startup. For MVP, a simple approach is: on bot startup, fetch all content from the database (maybe only those with `body` text, skipping purely category nodes), generate embeddings for each, and upsert them into Qdrant. Each vector point should carry the content’s ID (use that as the Qdrant point ID or store it in payload). Qdrant allows storing a payload JSON with each vector – we can store {id: content\_id, title: "..."} for reference, but just the ID is enough if we have the DB to retrieve details.

   * If the content database is large (50+ countries info might be fine, but if each has a lot of text paragraphs, consider chunking: possibly split long articles into multiple smaller chunks each with its own embedding for finer search granularity). A decision: if articles are long, splitting by paragraph or section improves search (the query might match a specific paragraph). For now, if content items are reasonably sized, we can embed the whole `body`. If they are very large, implement a simple split (like each paragraph as a separate vector with the same content ID reference). That would require adjusting how we handle search results (multiple results might come from one article). To keep it simple, assume one embedding per content item.
4. **Semantic Search Handler:** Determine how users will invoke search. Two possibilities:

   * Implicit: If the bot receives a message that is not a command and not a navigation callback – treat it as a search query. This is user-friendly: they just type a question or keywords, and the bot responds with results.
   * Explicit: Provide a `/search <query>` command. But typing the command every time is less convenient.
     We can implement both: if a user message arrives and we’re not in an admin conversation or other state, assume it’s a search query. (We should ensure no conflict with other features; since navigation uses only inline buttons/callbacks, free-text is free to use for search).
     Now, implement the search logic:
   * When a query comes in (e.g., user types "Что нужно для поездки во Францию?"), call `generate_embedding(query_text)` to get its vector.
   * Use the Qdrant client to query the "content\_vectors" collection for the nearest matches. e.g., `qdrant.search(collection_name="content_vectors", query_vector=query_embedding, top=5, limit=5)` – this will return up to 5 points with similarity scores.
   * Filter or examine results: if the top result has a low similarity (meaning query didn’t match well), you might decide to return "No relevant information found." But likely with semantic search, some result will always come; consider a threshold if possible (some vector DBs allow specifying a minimum score).
   * For each result, retrieve the content ID (from payload or using point ID if we set point ID = content\_id). Use our DB DAO to fetch those content records.
   * Decide how to present results. Options:

     * If the content pieces are short (a paragraph), we could return multiple hits. But if they are full articles, returning 5 full articles is not ideal in chat format. Perhaps just take the top 1 or top 3.
     * We can send the title of the top match and a short excerpt of the content, with an inline button "📖 Read more" that, when clicked, opens that content in the menu (we can craft a callback that navigates to that item’s menu path). Since our navigation already can open an item by id (in Milestone 1, we used callback data "open\_<id>"), we can reuse that: just send a message like: "*Top result:* France Travel Requirements – ...snippet... 【Read More】", where the "Read More" is an inline button with callback\_data "open\_<id>" for that content. If needed, we might need to handle that the callback from a search result might come without a parent context – but since our handler will fetch by id and show it, it should be fine.
     * If multiple results are shown, we could list 1, 2, 3 with their titles as separate buttons or as a numbered list in text. Perhaps simpler: show only the best match by default. The admin might prefer multiple results – but in Telegram UX, a single answer per query is often cleaner. We can refine based on testing.
   * If user enters another query, just do the next search. (No special state to clear, unless we mistakenly left them in an FSM – ensure that normal user messages don't conflict with admin FSM or others. For example, if an admin is in the middle of an add content FSM, maybe treat their messages accordingly. For normal users, no FSM involved, so it’s straightforward).
   * Edge case: If the query seems like a command (starts with "/"), we should not treat as search. Aiogram will route it to a command handler if exists, or ignore if unknown command. That’s fine.
5. **Update Admin Workflows for Search:** When content is added or edited by admin, we must also update the vector index. Specifically:

   * On adding a content with a body, after saving to DB, immediately generate its embedding and upsert to Qdrant. The Qdrant upsert call can be done asynchronously. If performance is a concern (embedding generation could take a second or two), you might do it in background: for example, respond to admin that content is added, and do the embedding index in a `create_task` or separate thread so as not to delay the bot. However, simplicity might be fine if it's quick.
   * On editing content: if the text is changed, regenerate the embedding and update the vector in Qdrant (use the content ID to overwrite the old vector). If only the title changed, the embedding need not change (since search focuses on body), but it might still be fine to update in case title has some info.
   * On deletion: remove the vector from Qdrant to keep the index clean (qdrant-client has a delete by ID or filter function). Not critical if it stays (it just means search might return an ID that no longer exists in DB, which would be bad), so definitely remove it. We can enforce referential integrity: e.g., delete vector first or catch exceptions if a search finds a non-existent content and handle gracefully.
6. **Testing Semantic Search:**

   * **Unit Test Embedding Function:** If using an external API, mock `generate_embedding` to return a known vector for known inputs (to keep tests deterministic and not rely on external calls). If using a local model, we might still want to avoid the heavy computation in tests – possibly monkeypatch it to return a simple vector or something. For instance, for testing, implement a dummy embedder that returns e.g. \[len(text)] or some simplistic encoding just to simulate different outputs.
   * **Integration Test with Qdrant:** Ideally, have Qdrant running in test environment (perhaps use Docker or a fixture to start Qdrant). Alternatively, mock the Qdrant client as well. For a simpler approach, treat the search logic as a black box: you could set up a scenario with known content entries and pre-computed small embeddings (like 2D vectors) and use a fake in-memory search (e.g., brute-force Python search) to simulate Qdrant. But that deviates from actual behavior. A more direct but heavy test: use Qdrant's in-memory mode by running it (since Qdrant is efficient, a small test collection search is quick). If doing so, ensure tests clean up (delete the collection after, or even easier, use a different collection name for tests).
   * **Test search result correctness:** Insert a few content items in the test DB with known text, index them. Then craft a query that should match one of them strongly. Assert that the top result’s ID matches expected content. For example, content A has body "Apple fruit", content B "Banana fruit". Query "apple" should return A. This ensures that our integration of generate\_embedding + qdrant.search + result parsing works end-to-end (given the embedding model, one may have to trust it semantically works; the test can be coarse).
   * **Test user input handling:** Simulate a user sending a message "XYZ" by directly calling the message handler for non-command text. If we set up Aiogram handlers, perhaps use `@user_router.message()` with no filters to catch all text. We should test that this handler triggers search and returns a message. In a unit test, we might call that handler with a dummy `Message` object (with text set to something) and then verify that the bot attempted to send a response containing an expected title or snippet. This might require mocking the bot’s `send_message`. Aiogram allows injecting a dummy bot or using the test library to capture outgoing messages. At minimum, we can structure our search logic as a function that given a query returns a result (string or data), and test that function.
   * **Manual Test:** After implementing, run the bot and try some queries. For example, if content exists about "France visa requirements", try asking (in Russian) a question that should retrieve that. See if the bot responds quickly and with a relevant answer. Also test queries that have no good match – ensure the bot doesn’t crash. We might implement a fallback like "Извините, ничего не найдено по вашему запросу." if all similarity scores are below a threshold. Tune as needed.
7. **Performance Considerations:** This feature can be resource-intensive. Keep in mind:

   * Embedding generation can be slow; if using OpenAI API, there is network latency (\~100-300ms). If using a local model, initial loading might be slow (several seconds to load model into memory) but after that, each inference could be \~100ms or more depending on hardware and length. Monitor the time and perhaps limit query length or do minimal preprocessing (like truncate queries to a certain length).
   * Qdrant search is fast (milliseconds) even for thousands of vectors, and can scale horizontally if needed. If the content base grows huge (many thousands of items or chunks), Qdrant allows adding more replicas or shards to handle volume. The design is already using an external service, which is good for scaling.
   * If high query rate is expected, and each query does heavy work, consider queueing or debouncing user queries (for example, if a user sends multiple queries quickly, handle sequentially or only last). This is more of an app logic consideration. In most cases, users will wait for an answer before asking again.

By the end of Milestone 3, users can either navigate via menus or simply ask a question to retrieve information. The bot essentially becomes both a browsing interface and a Q\&A interface over the content.

## Milestone 4: Deployment, CI/CD Refinement, and Scalability

*In this final milestone, we focus on making the application production-ready. We will finalize Docker deployment, improve the CI pipeline with tests, and consider scalability and maintenance aspects such as monitoring and best practices.*

1. **Docker Deployment Setup:** Finalize the Docker configuration for all services. In the `docker-compose.yml`, ensure the following:

   * **PostgreSQL Service:** Use an official Postgres image (e.g., `postgres:15-alpine`). Mount a volume for data persistence. Set environment variables for POSTGRES\_USER, POSTGRES\_PASSWORD, POSTGRES\_DB. If Alembic migrations need to run, we might handle that separately (either run Alembic from the bot container on startup or have a manual step).
   * **Qdrant Service:** Add `qdrant/qdrant` image (for example, `qdrant/qdrant:v1.3.5` or latest). Expose its default port (6333) to the bot container. For production, you might not expose it publicly, just within the Docker network. Qdrant stores data in a volume (you can mount one to persist the index between restarts).
   * **Bot Service:** Build from our Dockerfile. In the Dockerfile, base on a slim Python image (e.g., `python:3.11-slim`). Copy only necessary files. Install requirements (consider using a requirements.txt and `pip install --no-cache-dir -r requirements.txt`). If using any native dependencies (like for numpy or sentence-transformers), you might need to include those or use a slightly larger image. Use multistage build if needed to compile and then copy the final. Set the entrypoint to run the bot (e.g., `python -m bot.main` or similar).
   * Set up dependency links: The bot service should depend on Postgres and Qdrant (Docker Compose `depends_on` to ensure they start first). However, our bot should also handle retrying connections (e.g., if bot starts before Postgres is ready, SQLAlchemy should retry or we add a startup delay). We can implement a simple retry loop on startup for DB and Qdrant connection.
   * Use environment variables in Compose to pass the config (e.g., `DATABASE_URL=postgresql+asyncpg://user:pass@postgres:5432/dbname`, `QDRANT_HOST=qdrant` etc., `BOT_TOKEN` etc.). These will be picked up by our config system (Pydantic settings).
2. **Running Migrations in Deployment:** We need to apply Alembic migrations when deploying. Two approaches:

   * **Manual:** Document in README that one should run `alembic upgrade head` before starting the bot the first time (or whenever the schema changes).
   * **Automated:** One can bake migration into the bot startup (for example, have the bot run Alembic programmatically or run an `alembic upgrade` command in entrypoint). Alternatively, use a separate service or a one-time init container for migrations. To keep things simple, we can add to the entrypoint script: attempt to run migrations (ensuring the database is reachable). Alembic can be invoked via its API or by shell command. Wrap it so that if it fails (e.g., DB not up yet), wait and retry a couple times. This ensures the DB schema is up-to-date.
3. **Enhance CI Pipeline:** Now that all features are in place, expand the CI tests:

   * Ensure that running `pytest` in CI actually sets up a test database (perhaps use SQLite for simplicity in CI, or spin up a Postgres service in CI workflow using services). If using SQLite for tests (with SQLAlchemy), most logic can be tested except some Postgres-specific things. Since our use of Postgres isn't extremely specific (mostly standard SQL), SQLite might suffice for tests to keep CI simple. Alternatively, add a service `postgres` in GitHub Actions yaml and set `DATABASE_URL` accordingly for tests.
   * If feasible, also run an integration test for search in CI: spin up Qdrant in CI workflow (as a service container). This might be advanced, but if not, just rely on unit tests with mocks for search in CI.
   * Add steps in CI to build the Docker image (to ensure Dockerfile is correct). We can build the image on each push to main, and possibly even push to a registry if one is set up (not required for MVP, but noted). At least test that `docker build` succeeds. This catches Dockerfile issues early.
   * Maintain code quality: The CI already runs Black/Ruff. We might also add **mypy** for static type checking if desired (ensuring our async code and types are correct). This adds to maintainability but is optional.
4. **Volume-Aware Architecture & Scaling:** With 50+ European countries worth of content and potentially many users:

   * **Bot Scalability:** One instance of the bot (due to Telegram architecture) will handle all updates. Aiogram on a single process can handle thousands of concurrent users if I/O-bound. To scale beyond one process (multi-core or multiple machines), Telegram webhooks can be used with a load balancer, but that gets complex (each update might be delivered to any instance, so they need shared state if any; we mostly keep state in DB, so that’s fine). For now, plan to run one instance on a decent server. Monitor CPU/Mem usage; if embedding search becomes a bottleneck (CPU heavy), consider moving embedding generation to a separate service or using asynchronous task queue (like Celery or APScheduler jobs). For example, if real-time search is slow, one could send an immediate "Searching..." message and complete the search a second later. Our design already tries to offload heavy tasks (embedding, DB queries which are async).
   * **Database Scaling:** Postgres can handle the reads/writes of this application easily at our scale. Use connection pooling (SQLAlchemy’s engine does pooling by default; with asyncpg it's efficient). Ensure indices on foreign keys (parent\_id index to speed up `get_children`). If usage grows (many queries per second), we could add read replicas, but likely unnecessary early on.
   * **Qdrant Scaling:** Qdrant is built to scale horizontally by adding nodes or using its distributed mode. If we have tens of thousands of vectors, ensure adequate memory or use Qdrant’s disk storage mode (it has in-memory vs memmap options). We might not need to adjust anything for initial launch, just be aware that we can add resources or enable multi-node Qdrant if needed.
   * **Caching and Optimization:** To handle high volume, consider caching frequently accessed data in memory. For example, the top-level menu is likely accessed often; we could cache `get_children(None)` results on startup (and refresh if content changes). Similarly, if certain popular categories are hit often, cache them. Python’s asyncio + global cache dict is fine since we have a single process. Invalidate or update the cache in admin add/edit/delete handlers to keep it up to date. This reduces database load and speeds up responses.
   * **Asynchronous Best Practices:** Go through the code and ensure no blocking calls in the event loop: e.g., if using a local embedding model, wrap it in an `run_in_executor` to not block; ensure database operations are `await`ed properly and using async connections. Use Aiogram’s ability to handle asyncio tasks for parallelism if needed. Avoid long-running loops or sleep in handlers. If any heavy computation must occur, consider using background tasks or scheduling.
   * **Error Handling:** Add robust error handling especially for external components: if Qdrant query fails or times out, catch exceptions and maybe notify the user “Search currently unavailable, please try later.” Similarly, if DB operations fail (e.g., DB down), the bot should catch and log instead of crashing the whole bot. Aiogram’s middleware or global error handler can be used to log exceptions. Perhaps integrate a monitoring/alert (for instance, use Sentry or simple logs).
   * **Logging & Monitoring:** In production, run the bot with an appropriate log level (INFO for normal operation, DEBUG for troubleshooting). Log important events (like "Admin X added content Y"). If possible, aggregate logs or set up alerts for errors. This helps maintain reliability with many users.
5. **Documentation & Maintenance:** Update the README/documentation with instructions for deployment (how to use Docker Compose to start the services, how to run migrations, etc.). Document environment variables needed (Bot token, DB creds, etc.). Also write usage instructions: how admins can manage content (list the admin commands and how to find content IDs or use the interface), and how users can interact (maybe a short help summary). This ensures a smooth hand-off to whoever will operate the bot.

   * Additionally, note the current limitation (only Russian language) and perhaps how the design could accommodate future languages (e.g., by adding a language field to content and having separate menus per language, or deploying separate bots per language). Since initial launch is Russian, no action needed now, but the data model and code structure (if well-organized) should allow extension.
6. **Final Testing:** Before launch, do a full integration test:

   * Run the whole stack via Docker Compose on a staging environment. Use a test Telegram bot token to interact. Populate the DB with a substantial amount of content (e.g., all 50 countries info) to simulate real usage volume. Test navigation deeply (all levels), test search queries for various topics, test admin adding new content on the fly, etc.
   * Observe performance (latency of responses). If any slowness is noticed (for example, embedding search taking too long), consider remedies like those mentioned (background tasks or user feedback while waiting).
   * Ensure the bot remains stable under continuous use. Possibly use a bot testing framework or even just a script to simulate multiple queries to see if any rate limits or bottlenecks occur.
   * All tests and linters should be green in CI, indicating code quality is maintained.

By following these milestones, we incrementally build a robust Telegram bot with modular components: content navigation, admin management, and semantic search. Each phase ensures the foundation is solid (with tests and style checks) before adding the next feature. The end result will be a maintainable, scalable bot system – ready for a multi-country launch and easy to extend or refine in the future.
